{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *D*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Marc Bickel*\n",
    "* *Cyil Cadoux*\n",
    "* *Emma Lejal*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from utils import load_json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the preprocessed version of the courses, that we save in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_courses = load_json('data/preProcessedCourses.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionnary which gives the number of occurences per word (named wordCount). \n",
    "***\n",
    "We also give a unique identifier to every word (uid field in wordCount). \n",
    "***\n",
    "We create the reverse dictionnary to have the correspondance of the id to a word (ie idToWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCount = {}\n",
    "idToWord = {}\n",
    "unique_id = 0\n",
    "for c in processed_courses:\n",
    "    listOfWords = c['description'].split(' ')\n",
    "    for w in listOfWords:\n",
    "        try :\n",
    "            wordCount[w]['count'] +=1\n",
    "        except KeyError :\n",
    "            wordCount[w] = {'count' : 1, 'uid' : unique_id}\n",
    "            idToWord[unique_id] = w\n",
    "            unique_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7861"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size of vocabulary = last unique_id (it is incremented one more time at the end so we subtract 1)\n",
    "unique_id -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that given a course will return the id and the vector associated with this course. \n",
    "* * *\n",
    "The vector is for each word in wordCount, how many time it appears in the description of the given course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def course_vector(course):\n",
    "    id = course['courseId']\n",
    "    counts = defaultdict(int)\n",
    "    for token in course['description'].split(' '):\n",
    "        token_id = wordCount[token]['uid']\n",
    "        counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    keys =[x[0] for x in counts]\n",
    "    values = [x[1] for x in counts]\n",
    "    return (id, Vectors.sparse(unique_id -1, keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of use of the course_vector function with the Internet Analytics course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('COM-308',\n",
       " SparseVector(7861, {10: 2.0, 21: 3.0, 42: 2.0, 63: 6.0, 78: 2.0, 99: 2.0, 108: 2.0, 117: 2.0, 125: 1.0, 135: 1.0, 142: 1.0, 186: 1.0, 204: 1.0, 230: 2.0, 292: 2.0, 318: 1.0, 382: 4.0, 401: 1.0, 420: 1.0, 448: 1.0, 455: 1.0, 458: 2.0, 470: 2.0, 533: 1.0, 534: 1.0, 537: 2.0, 654: 1.0, 688: 1.0, 842: 2.0, 875: 5.0, 967: 1.0, 1113: 2.0, 1119: 1.0, 1174: 1.0, 1239: 1.0, 1280: 1.0, 1284: 1.0, 1398: 5.0, 1426: 2.0, 1427: 5.0, 1428: 5.0, 1429: 2.0, 1430: 1.0, 1431: 1.0, 1432: 1.0, 1433: 1.0, 1434: 2.0, 1435: 2.0, 1436: 2.0, 1437: 1.0, 1438: 4.0, 1439: 2.0, 1440: 1.0, 1441: 2.0, 1442: 1.0, 1443: 1.0, 1444: 1.0, 1445: 1.0, 1446: 1.0, 1447: 1.0, 1448: 1.0}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_vector(processed_courses[43])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the corpus of documents with the result of course_vector for every course, associated with an integer id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "doc_uid = 0\n",
    "for c in processed_courses:\n",
    "    id, vect = course_vector(c)\n",
    "    documents.append([doc_uid, vect])\n",
    "    doc_uid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = LDA.train(rdd, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = lda_model.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_topics(k, topics):\n",
    "    for i in range(k):\n",
    "        topicWords = []\n",
    "        for j in range(10):\n",
    "            topicWords.append(idToWord[topics[i][0][j]])\n",
    "        print(i+1, \" : \", topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['treatment', 'manag', 'environment', 'physic', 'mass', 'role', 'impact', 'risk', 'wast', 'explain']\n",
      "2  :  ['architectur', 'robot', 'polici', 'case', 'innov', 'organ', 'water', 'assist', 'urban', 'practic']\n",
      "3  :  ['optim', 'linear', 'stochast', 'data', 'deriv', 'statist', 'supervis', 'continu', 'price', 'financi']\n",
      "4  :  ['data', 'algorithm', 'engin', 'comput', 'linear', 'statist', 'practic', 'assign', 'function', 'space']\n",
      "5  :  ['chemic', 'thermodynam', 'phase', 'properti', 'state', 'reaction', 'metal', 'physic', 'polym', 'transfer']\n",
      "6  :  ['biolog', 'paper', 'molecular', 'protein', 'discuss', 'research', 'literatur', 'chemic', 'prepar', 'note']\n",
      "7  :  ['mechan', 'physic', 'magnet', 'flow', 'properti', 'laser', 'chemistri', 'electron', 'reaction', 'organ']\n",
      "8  :  ['imag', 'signal', 'circuit', 'digit', 'comput', 'filter', 'visual', 'code', 'analog', 'cod']\n",
      "9  :  ['energi', 'network', 'power', 'manag', 'object', 'integr', 'comput', 'level', 'convers', 'languag']\n",
      "10  :  ['optic', 'microscopi', 'mechan', 'electron', 'light', 'physic', 'forc', 'stabil', 'sensor', 'tissu']\n"
     ]
    }
   ],
   "source": [
    "print_topics(10, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects for topics\n",
    "1. Environmental sciences\n",
    "2. Architecture\n",
    "3. Mathematics\n",
    "4. Communication Systems\n",
    "5. Chemistry\n",
    "6. Biology\n",
    "7. Physics\n",
    "8. Signal Processing\n",
    "9. ?\n",
    "10. Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying alpha means varying docConcentration.\n",
    "***\n",
    "There default value is -1.0\n",
    "***\n",
    "We set the seed manually to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model_varying_alpha = LDA.train(rdd, k=10, docConcentration = 1.99 , topicConcentration = 1.01, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_topics = lda_model_varying_alpha.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['risk', 'market', 'price', 'innov', 'financi', 'financ', 'manag', 'econom', 'deriv', 'option']\n",
      "2  :  ['chemistri', 'magnet', 'physic', 'robot', 'comput', 'organ', 'properti', 'reaction', 'drug', 'solid']\n",
      "3  :  ['research', 'data', 'manag', 'engin', 'inform', 'object', 'architectur', 'team', 'perform', 'case']\n",
      "4  :  ['signal', 'function', 'imag', 'recommend', 'cancer', 'rate', 'set', 'type', 'filter', 'acoust']\n",
      "5  :  ['linear', 'algorithm', 'comput', 'statist', 'data', 'algebra', 'optim', 'numer', 'space', 'signal']\n",
      "6  :  ['mechan', 'electron', 'properti', 'sensor', 'surfac', 'polym', 'micro', 'physic', 'measur', 'microscopi']\n",
      "7  :  ['energi', 'circuit', 'electron', 'power', 'thermodynam', 'chemic', 'integr', 'quantum', 'state', 'semiconductor']\n",
      "8  :  ['energi', 'data', 'optim', 'random', 'continu', 'network', 'chain', 'practic', 'function', 'map']\n",
      "9  :  ['optic', 'imag', 'biolog', 'laser', 'paper', 'research', 'light', 'protein', 'experiment', 'literatur']\n",
      "10  :  ['flow', 'heat', 'engin', 'environment', 'water', 'physic', 'polici', 'fluid', 'reactor', 'simul']\n"
     ]
    }
   ],
   "source": [
    "print_topics(10, alpha_topics) # alpha = 1.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['risk', 'market', 'price', 'innov', 'financ', 'financi', 'econom', 'architectur', 'deriv', 'manag']\n",
      "2  :  ['magnet', 'chemistri', 'robot', 'physic', 'comput', 'research', 'topic', 'reaction', 'organ', 'drug']\n",
      "3  :  ['manag', 'data', 'object', 'research', 'engin', 'inform', 'architectur', 'perform', 'team', 'case']\n",
      "4  :  ['signal', 'imag', 'function', 'cancer', 'recommend', 'acoust', 'rate', 'fourier', 'type', 'topic']\n",
      "5  :  ['linear', 'algorithm', 'comput', 'data', 'statist', 'algebra', 'optim', 'signal', 'numer', 'space']\n",
      "6  :  ['electron', 'mechan', 'properti', 'sensor', 'physic', 'micro', 'microscopi', 'surfac', 'polym', 'film']\n",
      "7  :  ['energi', 'circuit', 'electron', 'power', 'thermodynam', 'chemic', 'integr', 'state', 'quantum', 'biolog']\n",
      "8  :  ['energi', 'data', 'optim', 'random', 'stochast', 'function', 'continu', 'chain', 'mass', 'network']\n",
      "9  :  ['optic', 'imag', 'biolog', 'laser', 'paper', 'protein', 'research', 'light', 'molecular', 'microscopi']\n",
      "10  :  ['flow', 'heat', 'energi', 'water', 'environment', 'engin', 'physic', 'fluid', 'polici', 'reactor']\n"
     ]
    }
   ],
   "source": [
    "print_topics(10, alpha_topics) # alpha = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of alpha : change the order of the words in the topics, we change the probability of the words appearing in each topic. When alpha is larger we push away the word that seem strange in the topic (eg, architecture is gone from topic 1 that looks like Financial topic, in topic 7, biology is also gone in a topic that looks like Physics). Indeed the words are globally all about science, so the distribution per topic will tend to be more uniform then sharp, as all the words are part of some bigger topic, they are not so different from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying beta means varying topicConcentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_varying_beta = LDA.train(rdd, k=10, docConcentration = 6.0 , topicConcentration = 1.99, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_topics = lda_model_varying_beta.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['risk', 'market', 'price', 'optim', 'financi', 'stochast', 'financ', 'measur', 'deriv', 'manag']\n",
      "2  :  ['magnet', 'robot', 'chemistri', 'practic', 'physic', 'properti', 'organ', 'research', 'reaction', 'state']\n",
      "3  :  ['architectur', 'data', 'inform', 'research', 'manag', 'tool', 'urban', 'engin', 'comput', 'algorithm']\n",
      "4  :  ['rate', 'signal', 'imag', 'data', 'manag', 'tool', 'function', 'case', 'optim', 'continu']\n",
      "5  :  ['data', 'algorithm', 'comput', 'linear', 'paper', 'optim', 'function', 'statist', 'inform', 'recommend']\n",
      "6  :  ['mechan', 'electron', 'stabil', 'properti', 'metal', 'data', 'microscopi', 'fractur', 'dynam', 'imag']\n",
      "7  :  ['energi', 'comput', 'circuit', 'power', 'data', 'quantum', 'engin', 'integr', 'thermodynam', 'inform']\n",
      "8  :  ['energi', 'optic', 'function', 'physic', 'engin', 'recommend', 'transvers', 'integr', 'properti', 'comput']\n",
      "9  :  ['optic', 'imag', 'laser', 'biolog', 'physic', 'light', 'protein', 'microscopi', 'spectroscopi', 'cover']\n",
      "10  :  ['energi', 'flow', 'engin', 'chemic', 'water', 'numer', 'heat', 'fluid', 'polici', 'biolog']\n"
     ]
    }
   ],
   "source": [
    "print_topics(10, beta_topics) # beta = 1.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['risk', 'market', 'optim', 'price', 'manag', 'financi', 'financ', 'decis', 'deriv', 'econom']\n",
      "2  :  ['magnet', 'chemistri', 'physic', 'properti', 'robot', 'solid', 'organ', 'drug', 'practic', 'state']\n",
      "3  :  ['engin', 'research', 'architectur', 'data', 'inform', 'team', 'manag', 'case', 'build', 'object']\n",
      "4  :  ['signal', 'function', 'continu', 'recommend', 'set', 'filter', 'analyz', 'rate', 'cancer', 'object']\n",
      "5  :  ['linear', 'algorithm', 'comput', 'data', 'statist', 'algebra', 'network', 'numer', 'space', 'optim']\n",
      "6  :  ['mechan', 'electron', 'properti', 'sensor', 'stabil', 'measur', 'polym', 'surfac', 'micro', 'sampl']\n",
      "7  :  ['energi', 'circuit', 'chemic', 'reaction', 'quantum', 'thermodynam', 'power', 'state', 'integr', 'electron']\n",
      "8  :  ['energi', 'protein', 'practic', 'data', 'transvers', 'network', 'assist', 'map', 'perform', 'resourc']\n",
      "9  :  ['optic', 'imag', 'biolog', 'laser', 'paper', 'microscopi', 'research', 'light', 'experiment', 'spectroscopi']\n",
      "10  :  ['flow', 'heat', 'engin', 'simul', 'environment', 'water', 'physic', 'polici', 'fluid', 'numer']\n"
     ]
    }
   ],
   "source": [
    "print_topics(10, beta_topics) # beta = 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of beta : Having a low beta is better than a higher one. For example, the stochastic word should not be in our Financial topic number 1 (it is with beta = 1.99, and not with beta = 1.01) and in topic 5 the word linear is first woith low beta but does not describe best the data science topic, it is indeed gone with a high beta. As the documents are courses, it is unlikely that we have many topics per document, so we take a low value of beta to get more sharp distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying k, alpha and beta to find best topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model_courses = LDA.train(rdd, k=8, docConcentration = 6.0 , topicConcentration = 1.01, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courses_topics = lda_model_courses.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['chemistri', 'reaction', 'organ', 'space', 'mechan', 'topic', 'inform', 'synthesi', 'data', 'includ']\n",
      "2  :  ['data', 'case', 'particip', 'manag', 'polici', 'set', 'inform', 'analyz', 'busi', 'solv']\n",
      "3  :  ['optim', 'risk', 'market', 'stochast', 'deriv', 'manag', 'price', 'financi', 'data', 'decis']\n",
      "4  :  ['energi', 'flow', 'research', 'physic', 'transfer', 'heat', 'chemic', 'experiment', 'mass', 'thermodynam']\n",
      "5  :  ['biolog', 'function', 'molecular', 'statist', 'protein', 'dynam', 'note', 'mechan', 'signal', 'molecul']\n",
      "6  :  ['optic', 'imag', 'linear', 'algorithm', 'quantum', 'laser', 'light', 'comput', 'stabil', 'signal']\n",
      "7  :  ['engin', 'electron', 'physic', 'properti', 'mechan', 'solid', 'integr', 'sensor', 'metal', 'semiconductor']\n",
      "8  :  ['architectur', 'comput', 'circuit', 'digit', 'object', 'tool', 'practic', 'perform', 'languag', 'softwar']\n"
     ]
    }
   ],
   "source": [
    "print_topics(8, courses_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a little bit less than the 10 topics (choose 8), as we see that some of them are unclear, we keep the high alpha value = 6.0 as we still have this vocabulary in a global science domain, so distribution tend to be more uniform. But we still have these courses description so it is unlikely that we mix completely all the topics, we then choose a low beta value = 1.01.\n",
    "***\n",
    "Subjects for topics\n",
    "1. Chemistry\n",
    "2. Management of technology\n",
    "3. Financial engineering\n",
    "4. Geology\n",
    "5. Biology\n",
    "6. Light physics\n",
    "7. Electricity engineering\n",
    "8. Computer sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the data of wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_rdd = sc.textFile(\"/ix/wikipedia-for-schools.txt\").map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the same steps to have the corpus to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One instance of wiki_rdd contains 'tokens', 'title' and 'page_id'\n",
    "wikiWordCount = {}\n",
    "idToWikiWord = {}\n",
    "unique_wiki_id = 0\n",
    "for page in wiki_rdd.collect():\n",
    "    for w in page['tokens']:\n",
    "        try :\n",
    "            wikiWordCount[w]['count'] +=1\n",
    "        except KeyError :\n",
    "            wikiWordCount[w] = {'count' : 1, 'uid' : unique_wiki_id}\n",
    "            idToWikiWord[unique_wiki_id] = w\n",
    "            unique_wiki_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 4178, 'uid': 11}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiWordCount['britain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'britain'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idToWikiWord[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494493"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size of vocabulary\n",
    "unique_wiki_id -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_vector(wiki):\n",
    "    counts = defaultdict(int)\n",
    "    for token in wiki['tokens']:\n",
    "        token_id = wikiWordCount[token]['uid']\n",
    "        counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    keys =[x[0] for x in counts]\n",
    "    values = [x[1] for x in counts]\n",
    "    return Vectors.sparse(unique_wiki_id -1, keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDocuments = []\n",
    "wiki_uid = 0\n",
    "for wiki in wiki_rdd.collect():\n",
    "    vect = wiki_vector(wiki)\n",
    "    wikiDocuments.append([wiki_uid, vect])\n",
    "    wiki_uid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_docs_rdd = sc.parallelize(wikiDocuments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the Dirichlet hyperparameters, here is our reasoning.\n",
    "- First Wikipedia has a lot of different topics (the idea of an enclyclopedia is indeed to define every topic), so we can increase k, we will have many interesting different topics (due to computations delay, we must keep k = 5)\n",
    "- As one page is supposed to be a definition, we think that the topicConcentrati<on is supposed to be sharp (beta low)\n",
    "- As the words are from a way more wider dictionnary that the scientific vocabulary of EPFL courses, we can put a lower value of docConcentration as the distributions of words will be sharper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model_wiki = LDA.train(wiki_docs_rdd, k=5, docConcentration = 1.3 , topicConcentration = 1.01, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_topics = lda_model_wiki.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_wiki_topics(k, topics):\n",
    "    for i in range(k):\n",
    "        topicWords = []\n",
    "        for j in range(10):\n",
    "            topicWords.append(idToWikiWord[topics[i][0][j]])\n",
    "        print(i+1, \" : \", topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  ['species', 'water', 'large', 'area', 'sea', 'years', 'north', 'found', 'called', 'small']\n",
      "2  :  ['war', 'american', 'â€“', 'british', 'united', 'states', 'government', 'march', 'january', 'french']\n",
      "3  :  ['city', 'world', 'south', 'â€“', 'time', 'united', 'national', 'international', 'government', 'years']\n",
      "4  :  ['number', 'system', 'theory', 'form', 'called', 'time', 'human', 'common', 'energy', '=']\n",
      "5  :  ['time', 'music', 'early', 'work', 'film', 'made', 'including', 'years', 'people', 'world']\n"
     ]
    }
   ],
   "source": [
    "print_wiki_topics(5, wiki_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics : \n",
    "1. nature\n",
    "2. geopolitics\n",
    "3. geography\n",
    "4. sciences\n",
    "5. culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are very global as k is small but reflect different parts of wikipedia"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
